{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6738dc",
   "metadata": {},
   "source": [
    "# Yvettes Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdb91ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from env import github_token, github_username\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import prepare as p\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk.sentiment\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import model as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b021eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sklearn.preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151ff17",
   "metadata": {},
   "source": [
    "### Acquire webscrape urls from repos and save to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8e899",
   "metadata": {},
   "source": [
    "Keytakaway from urls collection:\n",
    "After some exploration of urls in github search of most starred repos we noticed that the page number in the url was the only change from one page to another. Creating a loop to change page when web scraping will allow us to \n",
    "iterate through pages and get more urls into our csv file.\n",
    "\n",
    "We also notice that the link to the repo was under <a> class = \"v-align-middle\". We will add a forloop to scrape each link and remove the first \"/\" and save into csv file.\n",
    "    \n",
    "In total we were able to collect 1000 observation of urls 853 of which seem to be unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e2638",
   "metadata": {},
   "source": [
    "Pattern in urls:\n",
    "#'search?q=stars%3A%3E0&s=stars&type=Repositories'\n",
    "    #'search?p=2&q=stars%3A%3E0&s=stars&type=Repositories'\n",
    "    #'search?p=3&q=stars%3A%3E0&s=stars&type=Repositories'\n",
    "    \n",
    "#https://github.com/search?q=stars%3A%3E0&s=stars&type=Repositories\n",
    "#https://github.com/search?p=100&q=stars%3A%3E0&s=stars&type=Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631ba32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# function for scraping 1000 observations of urls\n",
    "\n",
    "#urls=[]\n",
    "#for i in range(0,100):\n",
    "#    url = f\"https://github.com/search?p={i}&q=stars%3A%3E0&s=stars&type=Repositories\"\n",
    "#    reqs = requests.get(url)\n",
    "#    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "#    time.sleep(2)\n",
    "#    for link in soup.find_all('a',class_=\"v-align-middle\"):\n",
    "#        link = re.sub(r'/', '', link.get('href'), count = 1)\n",
    "#        urls.append(link)\n",
    "#        time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls =pd.DataFrame(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e226b92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#urls.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25665008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert urls 1000 observations into a csv file\n",
    "\n",
    "#urls.to_csv('urls_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bdd5a7",
   "metadata": {},
   "source": [
    "### prep of  urls  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a8dbc",
   "metadata": {},
   "source": [
    "We will call on our saved urls_final.csv file to scrape the content of the repos of ReadMe text, main language of code. We will then save the information along with the repos name into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e264b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# convert urls into dataframe pull from csv file\n",
    "#urls_repo = pd.read_csv('urls_final.csv', index_col=0)\n",
    "#urls_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c33192",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_repo.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_repo.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_repo[460:470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_repo.iloc[468]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecfe148",
   "metadata": {},
   "source": [
    " scrapping functions where not functionin properly because url index 468 no longer existed, so it was dropped.\n",
    " \n",
    "0    eip-work/kuboard-press\n",
    "Name: 534, dtype: object\n",
    "urls_repo.drop([534], axis = 0, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f03792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_repo[460:470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_repo.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded548e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_repo.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2234293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_repo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9fc9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "urls_repo.iloc[466]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b788733",
   "metadata": {},
   "source": [
    "### acquire readmes from github using urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36806585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A module for obtaining repo readme and language data from the github API.\n",
    "Before using this module, read through it, and follow the instructions marked\n",
    "TODO.\n",
    "After doing so, run it like this:\n",
    "    python acquire.py\n",
    "To create the `data.json` file that contains the data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# TODO: Make a github personal access token.\n",
    "#     1. Go here and generate a personal access token: https://github.com/settings/tokens\n",
    "#        You do _not_ need select any scopes, i.e. leave all the checkboxes unchecked\n",
    "#     2. Save it in your env.py file under the variable `github_token`\n",
    "# TODO: Add your github username to your env.py file under the variable `github_username`\n",
    "# TODO: Add more repositories to the `REPOS` list below.\n",
    "\n",
    "REPOS = urls_repo['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfdd777",
   "metadata": {},
   "source": [
    "0-250 seem okay \n",
    "250-500  had an error\n",
    "460-470 had an error\n",
    "480-500 was good\n",
    "Repo index number 468 eip-work/kuboard-press no longer exists and is causing problems. # 468 will be dropped from csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1135e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad559606",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}\n",
    "\n",
    "if headers[\"Authorization\"] == \"token \" or headers[\"User-Agent\"] == \"\":\n",
    "    raise Exception(\n",
    "        \"You need to follow the instructions marked TODO in this script before trying to use it\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66927a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def github_api_request(url: str) -> Union[List, Dict]:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response_data = response.json()\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Error response from github api! status code: {response.status_code}, \"\n",
    "            f\"response: {json.dumps(response_data)}\"\n",
    "        )\n",
    "    return response_data\n",
    "\n",
    "\n",
    "def get_repo_language(repo: str) -> str:\n",
    "    url = f\"https://api.github.com/repos/{repo}\"\n",
    "    repo_info = github_api_request(url)\n",
    "    if type(repo_info) is dict:\n",
    "        repo_info = cast(Dict, repo_info)\n",
    "        if \"language\" not in repo_info:\n",
    "            raise Exception(\n",
    "                \"'language' key not round in response\\n{}\".format(json.dumps(repo_info))\n",
    "            )\n",
    "        return repo_info[\"language\"]\n",
    "    raise Exception(\n",
    "        f\"Expecting a dictionary response from {url}, instead got {json.dumps(repo_info)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_repo_contents(repo: str) -> List[Dict[str, str]]:\n",
    "    url = f\"https://api.github.com/repos/{repo}/contents/\"\n",
    "    contents = github_api_request(url)\n",
    "    if type(contents) is list:\n",
    "        contents = cast(List, contents)\n",
    "        return contents\n",
    "    raise Exception(\n",
    "        f\"Expecting a list response from {url}, instead got {json.dumps(contents)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_readme_download_url(files: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Takes in a response from the github api that lists the files in a repo and\n",
    "    returns the url that can be used to download the repo's README file.\n",
    "    \"\"\"\n",
    "    for file in files:\n",
    "        if file[\"name\"].lower().startswith(\"readme\"):\n",
    "            return file[\"download_url\"]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def process_repo(repo: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Takes a repo name like \"gocodeup/codeup-setup-script\" and returns a\n",
    "    dictionary with the language of the repo and the readme contents.\n",
    "    \"\"\"\n",
    "    contents = get_repo_contents(repo)\n",
    "    readme_download_url = get_readme_download_url(contents)\n",
    "    if readme_download_url == \"\":\n",
    "        readme_contents = \"\"\n",
    "    else:\n",
    "        readme_contents = requests.get(readme_download_url).text\n",
    "    return {\n",
    "        \"repo\": repo,\n",
    "        \"language\": get_repo_language(repo),\n",
    "        \"readme_contents\": readme_contents,\n",
    "    }\n",
    "\n",
    "\n",
    "def scrape_github_data() -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Loop through all of the repos and process them. Returns the processed data.\n",
    "    \"\"\"\n",
    "    return [process_repo(repo) for repo in REPOS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "#    try:\n",
    "#        data = scrape_github_data()\n",
    "#        json.dump(data, open(\"data.json\", \"w\"), indent=1)\n",
    "#    except: \n",
    "#        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2cfbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scrape repos for information.\n",
    "#df = scrape_github_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc31b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3ea9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29273e",
   "metadata": {},
   "source": [
    "Final data was scaped 1/11/2023 at 10pm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447417c4",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create csv to save data from github scrapped January 11, 2023 at 10pm\n",
    "#df.to_csv('readme_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4cb323",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('readme_df.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9473df7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3641e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.language == 'Java') | (df.language=='JavaScript') | (df.language=='Python') | (df.language=='TypeScript')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop =True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea92fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a9fa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53e94d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.language.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df)\n",
    "        df.dropna(inplace=True)\n",
    "        df = df[(df.language == 'Java') | (df.language=='JavaScript') | (df.language=='Python') | (df.language=='TypeScript')]\n",
    "        df.reset_index(drop =True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3066df2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94b7d369",
   "metadata": {},
   "source": [
    "### More Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddcccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply basic clean to content\n",
    "df['clean_text']= df.readme_contents.apply(p.basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b1ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokenize \n",
    "df['clean_text']= df.clean_text.apply(p.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce388e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "df['clean_text']= df.clean_text.apply(p.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28457aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "df['clean_text']= df.clean_text.apply(p.remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07ab23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b1322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e63b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "169/376"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229af57",
   "metadata": {},
   "source": [
    "# start here after scrapping\n",
    "get dataframe from csv and clean using prep functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5764fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire readme data\n",
    "df = pd.read_csv('readme_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe98f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "df = p.data_prep(df)\n",
    "\n",
    "# prepare text for exploration \n",
    "df = p.text_prep(df)\n",
    "\n",
    "\n",
    "# split data: train, validate and test\n",
    "train, validate, test = p.split_data(df, 'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb2ead6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4fbade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2515fcfd",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f31ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target):\n",
    "    '''\n",
    "    split_date takes in a dataframe  and target variable and splits into train , validate, test \n",
    "    and stratifies on target variable\n",
    "    \n",
    "    The split is 20% test 80% train/validate. Then 30% of 80% validate and 70% of 80% train.\n",
    "    Aproximately (train 56%, validate 24%, test 20%)\n",
    "    \n",
    "    returns train, validate, and test \n",
    "    '''\n",
    "    # split test data from train/validate\n",
    "    train_validate, test = train_test_split(df, test_size=.2, \n",
    "                                        random_state=123, \n",
    "                                        stratify=df[target])\n",
    "\n",
    "    # split train from validate\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                   random_state=123, \n",
    "                                   stratify=train_validate[target])\n",
    "\n",
    "                                   \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "733f4f14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train, validate, test = split_data(df,'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1fbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbd76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80e496",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e6297",
   "metadata": {},
   "source": [
    "### look at how many of each type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d9fcf",
   "metadata": {},
   "source": [
    "# Question is there a difference in sentimate by language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae0c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = a[['language','clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.reset_index(drop =True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0e14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sia = nltk.sentiment.SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.sentiment\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def get_sentiment(sentiment_df):\n",
    "    \n",
    "    # reindex dataframe\n",
    "    sentiment_df.reset_index(drop =True, inplace=True)\n",
    "    #create sntiment object\n",
    "    sia = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # create row id column\n",
    "    sentiment_df[\"row_id\"] =sentiment_df.index +1\n",
    "    \n",
    "    # create subsets\n",
    "    df_subset = sentiment_df[['row_id', 'clean_text']].copy()\n",
    "    \n",
    "    # set up empty dataframe for staging output\n",
    "    df1=pd.DataFrame()\n",
    "    df1['row_id']=['99999999999']\n",
    "    df1['sentiment_type']='NA999NA'\n",
    "    df1['sentiment_score']=0\n",
    "    \n",
    "    # run loop to calculate and save sentiment values\n",
    "    t_df = df1\n",
    "    for index,row in df_subset.iterrows():\n",
    "        scores = sia.polarity_scores(row[1])\n",
    "        for key, value in scores.items():\n",
    "            temp = [key,value,row[0]]\n",
    "            df1['row_id']=row[0]\n",
    "            df1['sentiment_type']=key\n",
    "            df1['sentiment_score']=value\n",
    "            t_df=t_df.append(df1)\n",
    "    #remove dummy row with row_id = 99999999999\n",
    "    t_df_cleaned = t_df[t_df.row_id != '99999999999']\n",
    "    #remove duplicates if any exist\n",
    "    t_df_cleaned = t_df_cleaned.drop_duplicates()\n",
    "    # only keep rows where sentiment_type = compound\n",
    "    t_df_cleaned = t_df[t_df.sentiment_type == 'compound']\n",
    "    \n",
    "    df_output = pd.merge(sentiment_df, t_df_cleaned, on='row_id', how='inner')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #generate mean of sentiment_score by period\n",
    " \n",
    "    dfg = df_output.groupby(['language'])['sentiment_score'].mean()\n",
    "    #create a bar plot\n",
    "    dfg.plot(kind='bar', title='Sentiment Score', ylabel='Mean Sentiment Score',\n",
    "         xlabel='Period', figsize=(6, 5))\n",
    "   \n",
    "    \n",
    "    return plt.show();\n",
    " \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c298c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_sentiment(sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fda178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3562757",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[]\n",
    "for i in range(len(sentimate_df)):    \n",
    "    score = sia.polarity_scores(sentimate_df.clean_text[i])\n",
    "    temp.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d68661",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimate_df[\"row_id\"] =sentimate_df.index +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ba3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = sentimate_df[['row_id', 'clean_text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ceed2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set up empty dataframe for staging output\n",
    "df1=pd.DataFrame()\n",
    "df1['row_id']=['99999999999']\n",
    "df1['sentiment_type']='NA999NA'\n",
    "df1['sentiment_score']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d151651",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing sentiment analysis...')\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "t_df = df1\n",
    "for index,row in df_subset.iterrows():\n",
    "    scores = sia.polarity_scores(row[1])\n",
    "    for key, value in scores.items():\n",
    "        temp = [key,value,row[0]]\n",
    "        df1['row_id']=row[0]\n",
    "        df1['sentiment_type']=key\n",
    "        df1['sentiment_score']=value\n",
    "        t_df=t_df.append(df1)\n",
    "#remove dummy row with row_id = 99999999999\n",
    "t_df_cleaned = t_df[t_df.row_id != '99999999999']\n",
    "#remove duplicates if any exist\n",
    "t_df_cleaned = t_df_cleaned.drop_duplicates()\n",
    "# only keep rows where sentiment_type = compound\n",
    "t_df_cleaned = t_df[t_df.sentiment_type == 'compound']\n",
    "print(t_df_cleaned.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d933ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge dataframes\n",
    "df_output = pd.merge(sentimate_df, t_df_cleaned, on='row_id', how='inner')\n",
    "print(df_output.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd6a11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_output[[\"sentiment_score\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f137aff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#generate mean of sentiment_score by period\n",
    "dfg = df_output.groupby(['language'])['sentiment_score'].mean()\n",
    "#create a bar plot\n",
    "dfg.plot(kind='bar', title='Sentiment Score', ylabel='Mean Sentiment Score',\n",
    "         xlabel='Period', figsize=(6, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f93b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ab0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.kdeplot(df_output[df_output.language == 'JavaScript'].sentiment_score, label = 'JavaScript')\n",
    "sns.kdeplot(df_output[df_output.language == \"Python\"].sentiment_score, label = 'Python')\n",
    "sns.kdeplot(df_output[df_output.language == \"Java\"].sentiment_score, label = 'Java')\n",
    "sns.kdeplot(df_output[df_output.language == \"TypeScript\"].sentiment_score, label = 'TypeScript')\n",
    "plt.legend(['JavaScript', 'Python','Java','TypeScript']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf85b4f",
   "metadata": {},
   "source": [
    "# Are the top 10 trigrams/bigrams the same accross languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcf3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "javaScript_words = ' '.join(train[train.language == 'JavaScript'].clean_text).split(' ')\n",
    "java_words = ' '.join(train[train.language == 'Java'].clean_text).split(' ')\n",
    "python_words = ' '.join(train[train.language == 'Python'].clean_text).split(' ')\n",
    "typeScript_words = ' '.join(train[train.language == 'TypeScript'].clean_text).split(' ')\n",
    "all_words = ' '.join(train.clean_text).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b48803",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "javaScript_bigrams = pd.Series(nltk.bigrams(javaScript_words ))\n",
    "java_bigrams = pd.Series(nltk.bigrams(java_words))\n",
    "python_bigrams = pd.Series(nltk.bigrams(python_words))\n",
    "typeScript_bigrams = pd.Series(nltk.bigrams(typeScript_words))\n",
    "all_bigrams= pd.Series(nltk.bigrams(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319bf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b91e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ngrams(words,top_num=10,n_grams=2 ):\n",
    "    top_ngrams = (pd.Series(nltk.ngrams(words, n_grams))\n",
    "                      .value_counts()\n",
    "                      .head(top_num))\n",
    "    return top_ngrams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3e414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "javascript_top_3grams= top_ngrams(javaScript_words, 10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e9669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_top_3grams= top_ngrams(java_words, 10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_top_3grams= top_ngrams(python_words, 10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46530739",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeScript_top_3grams= top_ngrams(typeScript_words, 10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_3grams= top_ngrams(all_words, 10,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6196e361",
   "metadata": {},
   "source": [
    "# From the top 10 words per language,Is there shared words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c13091",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf367a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "JavaScript_freq = pd.Series(javaScript_words).value_counts()\n",
    "Java_freq = pd.Series(java_words).value_counts()\n",
    "Python_freq = pd.Series(python_words).value_counts()\n",
    "TypeScript_freq = pd.Series(typeScript_words).value_counts()\n",
    "All_words_freq = pd.Series(all_words).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = (pd.concat([JavaScript_freq, Java_freq, Python_freq, TypeScript_freq, All_words_freq], axis=1, sort=True)\n",
    "                .set_axis(['JavaScript', 'Java', 'Python', 'TypeScript', 'AllWords'], axis=1, inplace=False)\n",
    "                .fillna(0)\n",
    "                .apply(lambda s: s.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts['raw_count'] = word_counts.AllWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b79aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts['frequency'] = word_counts.raw_count / word_counts.raw_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts['augmented_frequency'] = word_counts.frequency / word_counts.frequency.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d2533",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba315d1",
   "metadata": {},
   "source": [
    "# MOdeling try 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f907ff9",
   "metadata": {},
   "source": [
    "### Create Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create object\n",
    "cv1 = CountVectorizer()\n",
    "# apply to data\n",
    "bag_of_words = cv1.fit_transform(train.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8570d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d475411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bow\n",
    "bow = pd.DataFrame(bag_of_words.todense())\n",
    "bow.columns = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf68a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ab36c",
   "metadata": {},
   "source": [
    "### create TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "bag_of_words = tfidf.fit_transform(train.clean_text)\n",
    "\n",
    "pd.DataFrame(bag_of_words.todense(), \n",
    "             columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72006759",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_features = pd.Series(\n",
    "    dict(\n",
    "        zip(\n",
    "            tfidf.get_feature_names(), tfidf.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56cdfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_features.sort_values(ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_features.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fecceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words with bigrams\n",
    "cv2 = CountVectorizer(ngram_range=(2, 2))\n",
    "bag_of_grams = cv2.fit_transform(train.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0851bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bow\n",
    "bow2 = pd.DataFrame(bag_of_grams.todense())\n",
    "bow2.columns = cv2.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2233bc",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d905a",
   "metadata": {},
   "source": [
    "### prep for model seperate X-train from y-target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39240e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.clean_text\n",
    "y_train = train.language\n",
    "\n",
    "X_validate = validate.clean_text\n",
    "y_validate = validate.language\n",
    "\n",
    "X_test = test.clean_text\n",
    "y_test = test.language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f8c0c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db7bc6",
   "metadata": {},
   "source": [
    "### cv bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv = CountVectorizer() Train\n",
    "X_bow = cv.fit_transform(X_train)\n",
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "tree.fit(X_bow, y_train)\n",
    "tree.score(X_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv = CountVectorizer()\n",
    "X_bow_val = cv.transform(X_validate)\n",
    "\n",
    "\n",
    "tree.score(X_bow_val, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8639eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    dict(\n",
    "    zip(cv.get_feature_names(), \n",
    "    tree.feature_importances_))).sort_values().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb46784",
   "metadata": {},
   "source": [
    "### cv2 bag of grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28016c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow2 = cv2.fit_transform(X_train)\n",
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "tree.fit(X_bow2, y_train)\n",
    "tree.score(X_bow2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35eb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv = CountVectorizer()\n",
    "X_bow_val2 = cv2.transform(X_validate)\n",
    "\n",
    "\n",
    "tree.score(X_bow_val2, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c189aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    dict(\n",
    "    zip(cv2.get_feature_names(), \n",
    "    tree.feature_importances_))).sort_values().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f90eb",
   "metadata": {},
   "source": [
    "# decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a30f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_countvectorizer(X_train):\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    return cv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree(x_train, y_train, x_validate, y_validate, x_test, y_test,cv):\n",
    "    '''\n",
    "    Function gets Decision Tree model accuracy on train and validate data set \n",
    "    ''' \n",
    "    # create decision tree model using defaults and random state to replicate results\n",
    "    tree = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "    # fit model on training data\n",
    "    X_bow = cv.fit_transform(x_train)\n",
    "    tree.fit(X_bow, y_train)\n",
    "    train_score= tree.score(X_bow, y_train)\n",
    "    \n",
    "    # Whatever transformations we apply to X_train need to be applied to X_test\n",
    "    X_bow_val = cv.transform(x_validate)\n",
    "    val_score =tree.score(X_bow_val, y_validate)\n",
    "\n",
    "    return train_score, val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5fe0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_tree(X_train, y_train, X_validate, y_validate, X_test, y_test,cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaabff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whatever transformations we apply to X_train need to be applied to X_test\n",
    "cv = CountVectorizer()\n",
    "X_bow = cv.fit_transform(X_train)\n",
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "tree.fit(X_bow, y_train)\n",
    "tree.score(X_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b801ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whatever transformations we apply to X_train need to be applied to X_test\n",
    "X_bow_val = cv.transform(X_validate)\n",
    "tree.score(X_bow_val, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5b4992",
   "metadata": {},
   "source": [
    "# Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c54b67c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forest(x_train, y_train, x_validate, y_validate, x_test, y_test,cv):\n",
    "    X_bow1 = cv1.fit_transform(X_train)\n",
    "    rf = RandomForestClassifier(max_depth =6, \n",
    "                            min_samples_leaf = 2, \n",
    "                            random_state=123)\n",
    "    rf.fit(X_bow1, y_train)\n",
    "    train_score = rf.score(X_bow1, y_train)\n",
    "    \n",
    "    # Whatever transformations we apply to X_train need to be applied to X_test\n",
    "    X_bow_val = cv1.transform(X_validate)\n",
    "    val_score =rf.score(X_bow_val, y_validate)\n",
    "    \n",
    "    return train_score, val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f456b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_forest(X_train, y_train, X_validate, y_validate, X_test, y_test,cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2034d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418bc580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whatever transformations we apply to X_train need to be applied to X_test\n",
    "cv1 = CountVectorizer()\n",
    "X_bow1 = cv1.fit_transform(X_train)\n",
    "rf = RandomForestClassifier(max_depth =6, \n",
    "                            min_samples_leaf = 2, \n",
    "                            random_state=123)\n",
    "rf.fit(X_bow1, y_train)\n",
    "rf.score(X_bow1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whatever transformations we apply to X_train need to be applied to X_test\n",
    "X_bow_val = cv1.transform(X_validate)\n",
    "rf.score(X_bow_val, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95292c",
   "metadata": {},
   "source": [
    "# Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dab795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn(x_train, y_train, x_validate, y_validate, x_test, y_test,cv):\n",
    "    X_bow1 = cv.fit_transform(X_train)\n",
    "    knn = KNeighborsClassifier(n_neighbors=6, weights='uniform')\n",
    "    knn.fit(X_bow, y_train)\n",
    "    train_score = knn.score(X_bow1, y_train)\n",
    "    \n",
    "    # Whatever transformations we apply to X_train need to be applied to X_test\n",
    "    X_bow_val = cv.transform(X_validate)\n",
    "    val_score =knn.score(X_bow_val, y_validate)\n",
    "    \n",
    "    return train_score, val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_knn(X_train, y_train, X_validate, y_validate, X_test, y_test,cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a6b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer()\n",
    "X_bow = cv2.fit_transform(X_train)\n",
    "knn = KNeighborsClassifier(n_neighbors=6, weights='uniform')\n",
    "knn.fit(X_bow, y_train)\n",
    "knn.score(X_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow_val = cv2.transform(X_validate)\n",
    "knn.score(X_bow_val, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1a431",
   "metadata": {},
   "source": [
    "# Start Here with models...Final Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f43535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,X_validate,y_validate, X_test, y_test = m.model_prep(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b3050cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = m.cv_countvectorizer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "603f4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree_Train,DecisionTree_Validate=m.get_tree(X_train,y_train,X_validate,y_validate, X_test,y_test,cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8961490f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KNN_Train,KNN_Validate = m.get_knn(X_train,y_train,X_validate,y_validate, X_test,y_test,cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2638006",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest_Train,RandomForest_Validate =m.get_forest(X_train,y_train,X_validate,y_validate, X_test,y_test,cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dd2daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Baseline Train', 'DecisionTree_Train', 'RandomForest_Train','KNN_Train','Baseline_Validate', 'DecisionTree_Validate', 'RandomForest_Validate','KNN_Validate']\n",
    "def make_stats_df():\n",
    "    '''\n",
    "    Function creates dataframe for results of pearsonsr statistical \n",
    "    test for all features.\n",
    "    '''\n",
    "    evaluate_df = pd.DataFrame()\n",
    "    evaluate_df['models'] = models\n",
    "    return evaluate_df\n",
    "\n",
    "def final_eval(train, validate, evaluate_df):\n",
    "    base_train = baseline_mean_errors(train,validate)\n",
    "    simp_train = lm_errors(train)\n",
    "    gen_train = glm_errors(train)\n",
    "    base_val = baseline_mean_errors(train,validate)\n",
    "    simp_val = lm_errors(validate)\n",
    "    gen_val = glm_errors(validate)\n",
    "\n",
    "\n",
    "    scores = [base_train, simp_train, gen_train, base_val, simp_val, gen_val]\n",
    "    evaluate_df['RMSE']=scores\n",
    "    \n",
    "    return evaluate_df\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32dde9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTree_Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest_Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN_Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baseline_Validate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DecisionTree_Validate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForest_Validate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN_Validate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  models\n",
       "0         Baseline Train\n",
       "1     DecisionTree_Train\n",
       "2     RandomForest_Train\n",
       "3              KNN_Train\n",
       "4      Baseline_Validate\n",
       "5  DecisionTree_Validate\n",
       "6  RandomForest_Validate\n",
       "7           KNN_Validate"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_stats_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7459e330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
